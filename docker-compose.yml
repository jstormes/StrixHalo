############################################################################
# StrixHalo LLM Server - Optimized for AMD Ryzen AI Max+ 395
#
# Features:
# - 1M token context window (full model capacity)
# - q8_0 KV cache quantization (quality + memory balance)
# - Vulkan GPU acceleration via Mesa RADV (Radeon 8060S)
# - OpenAI-compatible API on port 8091
#
# Download model first:
#   ./download-model.sh      # Q8 (~34GB, higher quality)
#   ./download-model-q4.sh   # Q4 (~18GB, smaller/faster)
#
# Usage:
#   docker compose up -d
#   curl http://localhost:8091/v1/chat/completions \
#     -H "Content-Type: application/json" \
#     -d '{"model": "qwen3-coder", "messages": [{"role": "user", "content": "Hello"}]}'
#
# Performance (after warmup):
#   Prompt: ~400-480 tok/s | Generation: ~35-40 tok/s
#
# Use with Qwen Code CLI (https://github.com/QwenLM/qwen-code):
#   export OPENAI_API_BASE=http://localhost:8091/v1
#   export OPENAI_API_KEY=not-needed
#   export OPENAI_MODEL=qwen3-coder
#   qwen
#
# Note: First request is slower due to Vulkan shader compilation
############################################################################

services:
  llm-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-vulkan:latest
    container_name: llm-vulkan
    hostname: llm-vulkan

    ports:
      - "8091:8091"

    environment:
      # Model configuration (download with ./download-model.sh or ./download-model-q4.sh)
      # Q8 model (default): Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q8_K_XL.gguf
      # Q4 model (smaller): Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf
      - MODEL_PATH=/models/Qwen3-Coder-30B-A3B-Instruct-1M-UD-Q4_K_XL.gguf
      - MODEL_ALIAS=qwen3-coder

      # GPU layers - Vulkan backend (999 = all layers on GPU)
      - GPU_LAYERS=999

      # Context window - reduced for Radeon 780M
      - CTX_SIZE=524288

      # Single slot for maximum context per request
      - PARALLEL=1

      # Performance optimizations
      - FLASH_ATTENTION=true
      - CONT_BATCHING=true

      # KV cache quantization: q8_0 for quality, q4_0 for max context
      # Options: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
      - KV_CACHE_TYPE=q4_0

      # Batch sizes
      - BATCH_SIZE=2048
      - UBATCH_SIZE=512

      # Keep model locked in RAM
      - MLOCK=true

    volumes:
      - ./models:/models:ro
      - llama-cache:/root/.cache

    # GPU access for Vulkan (uses /dev/dri only)
    devices:
      - /dev/dri:/dev/dri

    # Use numeric GIDs for video/render groups
    # Run `getent group video render` to find your system's GIDs
    group_add:
      - "44"    # video
      - "992"   # render (adjust if different on your system)

    # Memory limits (adjusted for 64GB system)
    deploy:
      resources:
        limits:
          memory: 50G
        reservations:
          memory: 30G

volumes:
  llama-cache:
    driver: local
